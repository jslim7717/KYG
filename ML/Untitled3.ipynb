{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "# SUBCLASS가 범주형이기 때문에 LabelEncoder 사용\n",
    "le_subclass = LabelEncoder()\n",
    "train['SUBCLASS'] = le_subclass.fit_transform(train['SUBCLASS'])\n",
    "\n",
    "# 특성 및 타겟 변수 분리\n",
    "X = train.drop(columns=['SUBCLASS', 'ID'])\n",
    "y_subclass = train['SUBCLASS']\n",
    "\n",
    "# 범주형 특징에 대해 원-핫 인코딩 수행\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns)\n",
    "\n",
    "### 1. RandomForest를 사용한 차원 축소 ###\n",
    "# RandomForest를 사용한 중요도 기반 차원 축소\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_encoded, y_subclass)\n",
    "\n",
    "# 중요도 누적값을 기준으로 중요한 피처만 선택\n",
    "feature_importances = rf.feature_importances_\n",
    "sorted_idx = feature_importances.argsort()[::-1]\n",
    "cumulative_importance = feature_importances[sorted_idx].cumsum()\n",
    "\n",
    "# 중요도 누적값 0.7 기준으로 피처 선택\n",
    "important_features_idx = sorted_idx[cumulative_importance <= 0.7]\n",
    "X_reduced = X_encoded.iloc[:, important_features_idx]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reduced, y_subclass, test_size=0.2, random_state=42)\n",
    "\n",
    "### 2. 최적화된 모델 설정 ###\n",
    "# XGBoost 모델 최적화된 파라미터로 설정\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=233,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.09066897671128973,\n",
    "    subsample=0.7445604876879595,\n",
    "    colsample_bytree=0.6030050347739594,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# RandomForest 모델 최적화된 파라미터로 설정\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=349,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# LightGBM 모델 최적화된 파라미터로 설정\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=259,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.040719770187575215,\n",
    "    subsample=0.9981840021295828,\n",
    "    colsample_bytree=0.9255086919552659,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression 모델\n",
    "lr_model = LogisticRegression(max_iter=200, random_state=42)\n",
    "\n",
    "### 3. Stacking 앙상블 ###\n",
    "# StackingClassifier 정의\n",
    "estimators = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('rf', rf_model),\n",
    "    ('lr', lr_model),\n",
    "    ('lgb', lgb_model)\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(),  # 최종 메타 모델\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# 스태킹 모델 훈련\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 세트에 대한 예측 및 성능 평가\n",
    "y_val_pred = stacking_model.predict(X_val)\n",
    "y_val_pred_proba = stacking_model.predict_proba(X_val)\n",
    "validation_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Validation Log Loss: {log_loss(y_val, y_val_pred_proba)}\")\n",
    "print(f\"Validation Macro F1 Score: {validation_macro_f1}\")\n",
    "\n",
    "# Inference\n",
    "test_X = test.drop(columns=['ID'])\n",
    "X_encoded_test = pd.get_dummies(test_X, columns=categorical_columns)\n",
    "\n",
    "# 누락된 원-핫 인코딩 컬럼을 맞추기 위해 train 데이터의 컬럼 기준으로 맞춤\n",
    "X_encoded_test = X_encoded_test.reindex(columns = X_encoded.columns, fill_value=0)\n",
    "\n",
    "# 테스트 데이터에서 중요한 피처만 선택\n",
    "X_reduced_test = X_encoded_test.iloc[:, important_features_idx]\n",
    "\n",
    "# 최종 예측 수행\n",
    "predictions = stacking_model.predict(X_reduced_test)\n",
    "original_labels = le_subclass.inverse_transform(predictions)\n",
    "\n",
    "# Submission\n",
    "submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "submission[\"SUBCLASS\"] = original_labels\n",
    "submission.to_csv('./submission_with_onehot_0.7.csv', encoding='UTF-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b1fa35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_680\\2990979672.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import Library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import Library\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ff063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4357d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# SUBCLASS가 범주형이기 때문에 LabelEncoder 사용\n",
    "le_subclass = LabelEncoder()\n",
    "train['SUBCLASS'] = le_subclass.fit_transform(train['SUBCLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8261981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 및 타겟 변수 분리\n",
    "X = train.drop(columns=['SUBCLASS', 'ID'])\n",
    "y_subclass = train['SUBCLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fef354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 특징에 대해 원-핫 인코딩 수행\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f725b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. RandomForest를 사용한 차원 축소 ###\n",
    "# RandomForest를 사용한 중요도 기반 차원 축소\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_encoded, y_subclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db31e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요도 누적값을 기준으로 중요한 피처만 선택\n",
    "feature_importances = rf.feature_importances_\n",
    "sorted_idx = feature_importances.argsort()[::-1]\n",
    "cumulative_importance = feature_importances[sorted_idx].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요도 누적값 0.7 기준으로 피처 선택\n",
    "important_features_idx = sorted_idx[cumulative_importance <= 0.7]\n",
    "X_reduced = X_encoded.iloc[:, important_features_idx]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reduced, y_subclass, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. 최적화된 모델 설정 ###\n",
    "# XGBoost 모델 최적화된 파라미터로 설정\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=233,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.09066897671128973,\n",
    "    subsample=0.7445604876879595,\n",
    "    colsample_bytree=0.6030050347739594,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# RandomForest 모델 최적화된 파라미터로 설정\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=349,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# LightGBM 모델 최적화된 파라미터로 설정\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=259,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.040719770187575215,\n",
    "    subsample=0.9981840021295828,\n",
    "    colsample_bytree=0.9255086919552659,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression 모델\n",
    "lr_model = LogisticRegression(max_iter=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Stacking 앙상블 ###\n",
    "# StackingClassifier 정의\n",
    "estimators = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('rf', rf_model),\n",
    "    ('lr', lr_model),\n",
    "    ('lgb', lgb_model)\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(),  # 최종 메타 모델\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# 스태킹 모델 훈련\n",
    "stacking_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 세트에 대한 예측 및 성능 평가\n",
    "y_val_pred = stacking_model.predict(X_val)\n",
    "y_val_pred_proba = stacking_model.predict_proba(X_val)\n",
    "validation_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Validation Log Loss: {log_loss(y_val, y_val_pred_proba)}\")\n",
    "print(f\"Validation Macro F1 Score: {validation_macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625460de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "test_X = test.drop(columns=['ID'])\n",
    "X_encoded_test = pd.get_dummies(test_X, columns=categorical_columns)\n",
    "\n",
    "# 누락된 원-핫 인코딩 컬럼을 맞추기 위해 train 데이터의 컬럼 기준으로 맞춤\n",
    "X_encoded_test = X_encoded_test.reindex(columns = X_encoded.columns, fill_value=0)\n",
    "\n",
    "# 테스트 데이터에서 중요한 피처만 선택\n",
    "X_reduced_test = X_encoded_test.iloc[:, important_features_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f704352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 예측 수행\n",
    "predictions = stacking_model.predict(X_reduced_test)\n",
    "original_labels = le_subclass.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "submission[\"SUBCLASS\"] = original_labels\n",
    "submission.to_csv('./submission_with_onehot_0.7.csv', encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
