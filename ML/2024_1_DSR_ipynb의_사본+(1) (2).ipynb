{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. 성능평가지표 f1, AUC O\n",
        "2. voting(soft) x\n",
        "3. quantization o\n",
        "4. cnn 계열 모델(densenet, inception-v3, resnet18, 34, 50) o\n",
        "5. scheduler(reduceLRonPlateau) o\n",
        "6. 증강 x\n",
        "ppt\n",
        "1.스팩트로그램 변환 o\n",
        "2.split(val) o\n",
        "\n",
        "\n",
        "grid-search\n",
        "ablation=optuna"
      ],
      "metadata": {
        "id": "bSnHdxrkRKVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9Wo6fWD3b5F",
        "outputId": "aec9c868-78bd-4cc6-da3c-c1022c737650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 압축 해제 명령\n",
        "!unzip -qq \"/content/drive/MyDrive/dataset.zip\" -d dataset"
      ],
      "metadata": {
        "id": "wSdl7ARSQPVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 압축 해제 명령\n",
        "!unzip -qq \"/content/drive/MyDrive/dataset1.zip\" -d dataset1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCy5W-bAnI-0",
        "outputId": "e25e764b-c303-4277-9ebb-21af8ee7c662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/dataset1/val_spectrograms/0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inrWIhB70DU_",
        "outputId": "a39e9a68-3607-41bc-d240-7ed69e1f9887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_0019.png\tTRAIN_0897.png\tTRAIN_1687.png\tTRAIN_2357.png\tTRAIN_3346.png\tTRAIN_4472.png\n",
            "TRAIN_0061.png\tTRAIN_0961.png\tTRAIN_1737.png\tTRAIN_2414.png\tTRAIN_3357.png\tTRAIN_4482.png\n",
            "TRAIN_0092.png\tTRAIN_1018.png\tTRAIN_1759.png\tTRAIN_2419.png\tTRAIN_3452.png\tTRAIN_4502.png\n",
            "TRAIN_0095.png\tTRAIN_1046.png\tTRAIN_1797.png\tTRAIN_2431.png\tTRAIN_3569.png\tTRAIN_4509.png\n",
            "TRAIN_0260.png\tTRAIN_1107.png\tTRAIN_1813.png\tTRAIN_2444.png\tTRAIN_3576.png\tTRAIN_4513.png\n",
            "TRAIN_0315.png\tTRAIN_1110.png\tTRAIN_1814.png\tTRAIN_2445.png\tTRAIN_3633.png\tTRAIN_4522.png\n",
            "TRAIN_0322.png\tTRAIN_1204.png\tTRAIN_1827.png\tTRAIN_2516.png\tTRAIN_3648.png\tTRAIN_4543.png\n",
            "TRAIN_0442.png\tTRAIN_1288.png\tTRAIN_1839.png\tTRAIN_2558.png\tTRAIN_3652.png\tTRAIN_4622.png\n",
            "TRAIN_0617.png\tTRAIN_1317.png\tTRAIN_1985.png\tTRAIN_2659.png\tTRAIN_3710.png\tTRAIN_4650.png\n",
            "TRAIN_0657.png\tTRAIN_1327.png\tTRAIN_2010.png\tTRAIN_2661.png\tTRAIN_3764.png\tTRAIN_4686.png\n",
            "TRAIN_0682.png\tTRAIN_1396.png\tTRAIN_2015.png\tTRAIN_2771.png\tTRAIN_3792.png\tTRAIN_4687.png\n",
            "TRAIN_0740.png\tTRAIN_1410.png\tTRAIN_2096.png\tTRAIN_2832.png\tTRAIN_3836.png\tTRAIN_4688.png\n",
            "TRAIN_0755.png\tTRAIN_1452.png\tTRAIN_2116.png\tTRAIN_3009.png\tTRAIN_3951.png\tTRAIN_4718.png\n",
            "TRAIN_0758.png\tTRAIN_1466.png\tTRAIN_2141.png\tTRAIN_3049.png\tTRAIN_4073.png\tTRAIN_4904.png\n",
            "TRAIN_0774.png\tTRAIN_1511.png\tTRAIN_2222.png\tTRAIN_3057.png\tTRAIN_4093.png\tTRAIN_4919.png\n",
            "TRAIN_0777.png\tTRAIN_1519.png\tTRAIN_2223.png\tTRAIN_3101.png\tTRAIN_4094.png\tTRAIN_4957.png\n",
            "TRAIN_0782.png\tTRAIN_1596.png\tTRAIN_2240.png\tTRAIN_3130.png\tTRAIN_4251.png\tTRAIN_4963.png\n",
            "TRAIN_0790.png\tTRAIN_1602.png\tTRAIN_2266.png\tTRAIN_3141.png\tTRAIN_4275.png\n",
            "TRAIN_0848.png\tTRAIN_1638.png\tTRAIN_2316.png\tTRAIN_3172.png\tTRAIN_4280.png\n",
            "TRAIN_0854.png\tTRAIN_1657.png\tTRAIN_2340.png\tTRAIN_3215.png\tTRAIN_4379.png\n",
            "TRAIN_0861.png\tTRAIN_1659.png\tTRAIN_2352.png\tTRAIN_3315.png\tTRAIN_4414.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install librosa matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUabFXlSyN3_",
        "outputId": "2bcde102-3b16-4a3d-8459-5bb4e1c3adb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.11.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szXz9gX128X0",
        "outputId": "b79e7bb2-b403-4986-9062-292bdedf1158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch 관련 라이브러리\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
        "from torchsummary import summary\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# 일반 라이브러리\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        " #스팩트로그램\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# 스케쥴러, AUC\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xfzDkTlccXv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 관련 파라미터 모음 --> 자유롭게 변경하고, 추가해보세요.\n",
        "\n",
        "class Args():\n",
        "  data_type = \"2d\"\n",
        "  scheduler = \"reducelronplateau\"\n",
        "  model = \"resnet\"\n",
        "  n_class = 2\n",
        "  epoch = 50\n",
        "  phase = \"train\"\n",
        "  model_path = \"./model_weight_2d.pth\"\n",
        "\n",
        "args = Args()\n"
      ],
      "metadata": {
        "id": "6bDd5P_wcosC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 세트 관련 함수 --> 데이터 증강 기법을 적절하게 추가해보세요.\n",
        "class TimeMasking(object):\n",
        "    def __init__(self, T=40, max_masks=1):\n",
        "        self.T = T\n",
        "        self.max_masks = max_masks\n",
        "\n",
        "    def __call__(self, spec):\n",
        "        for _ in range(0, self.max_masks):\n",
        "            t = random.randrange(0, self.T)\n",
        "            t0 = random.randrange(0, spec.shape[1] - t)\n",
        "            spec[:, t0:t0+t] = 0\n",
        "        return spec\n",
        "\n",
        "class FrequencyMasking(object):\n",
        "    def __init__(self, F=30, max_masks=1):\n",
        "        self.F = F\n",
        "        self.max_masks = max_masks\n",
        "\n",
        "    def __call__(self, spec):\n",
        "        for _ in range(0, self.max_masks):\n",
        "            f = random.randrange(0, self.F)\n",
        "            max_f0 = spec.shape[0] - f\n",
        "            if max_f0 <= 0:\n",
        "                continue\n",
        "            f0 = random.randrange(0, max_f0)\n",
        "            spec[f0:f0+f, :] = 0\n",
        "        return spec\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, directory, transform=None):\n",
        "        self.directory = directory\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(os.listdir(directory))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(directory, class_name)\n",
        "            for image_name in os.listdir(class_dir):\n",
        "                self.samples.append((os.path.join(class_dir, image_name), self.class_to_idx[class_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.samples[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "PRZwjrQcc827"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 학습용 데이터셋 및 데이터 로더\n",
        "train_dataset = ImageDataset('/content/dataset1/train_spectrograms', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# 검증용 데이터셋 및 데이터 로더\n",
        "val_dataset = ImageDataset('/content/dataset1/val_spectrograms', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agLUtlB61jxS",
        "outputId": "ad14cb27-64cc-4387-f8b3-02ae0c778ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용 데이터셋의 크기 출력\n",
        "print(f\"학습 데이터 수는 {len(train_dataset)}개 입니다.\")\n",
        "\n",
        "# 검증용 데이터셋의 크기 출력\n",
        "print(f\"검증 데이터 수는 {len(val_dataset)}개 입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qDmKplpIZZ-",
        "outputId": "36bc2486-6fb6-4fca-a75c-0042071ca58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 수는 2800개 입니다.\n",
            "검증 데이터 수는 704개 입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 파일 경로\n",
        "image_path = '/content/dataset/train_spectrograms/0/TRAIN_0007.png'\n",
        "\n",
        "# 이미지 열기\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# 이미지 시각화\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # 축 표시 안함\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1wdCNx70QeAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(name, n_class, pretrained=True): # pretrained는 항상 False로 두어야 합니다. True로 학습할 경우 감점(-5)\n",
        "    if name == 'vgg16':\n",
        "        model = models.vgg16(pretrained=pretrained)\n",
        "        num_features = model.classifier[2].in_features\n",
        "        model.classifier[2] = nn.Linear(num_features, n_class)\n",
        "    elif name == 'resnet':\n",
        "        model = models.resnet18(pretrained=pretrained)\n",
        "        num_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_features, n_class)\n",
        "    elif name == 'densenet':\n",
        "        model = models.densenet121(pretrained=pretrained)\n",
        "        num_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(num_features, n_class)\n",
        "    elif name == 'inception_v3':\n",
        "        model = models.inception_v3(pretrained=pretrained)\n",
        "        # Replace the classifier with a fully connected layer with n_class outputs\n",
        "        model.fc = nn.Linear(model.fc.in_features, n_class)\n",
        "    return model\n",
        "\n",
        "resnet = get_model('resnet', args.n_class)\n",
        "vgg16 = get_model('vgg16', args.n_class)\n",
        "densenet = get_model('densenet', args.n_class)\n",
        "inception_v3 = get_model('inception_v3', args.n_class)\n",
        "\n",
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple1DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1000)  # 출력을 1000의 길이로 고정\n",
        "        self.fc = nn.Linear(64 * 1000, 6)  # Adaptive Pooling 이후의 출력 크기에 맞춰 조정\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.adaptive_pool(x)  # Adaptive Pooling 적용\n",
        "        x = x.view(-1, 64 * 1000)  # 적절한 크기로 플래트닝\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Simple2DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple2DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # 입력 채널 3, 출력 채널 64\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # 입력 채널 64, 출력 채널 128\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # 입력 채널 128, 출력 채널 256\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 2x2 풀링\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d(7)\n",
        "        self.fc = nn.Linear(256 * 7 * 7, 6)  # 최종 출력을 위한 완전 연결 계층\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 첫 번째 컨볼루션 + 풀링\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 두 번째 컨볼루션 + 풀링\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 세 번째 컨볼루션 + 풀링\n",
        "        x = self.adaptive_pool(x)  # Adaptive Pooling 적용\n",
        "        x = x.view(-1, 256 * 7 * 7)  # 플래트닝\n",
        "        x = self.fc(x)  # 완전 연결 계층\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDngiptQ5Qw",
        "outputId": "82097a37-fcae-427f-db14-5890dbf4bf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils에 포함되었던 함수들 + tic toc 추가. tic toc은 수정하지마세요.\n",
        "def visualize_audio_batch(audio_signals, labels):\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(10, 5))\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i >= 16:  # 16개의 오디오만 표시\n",
        "            break\n",
        "        ax.plot(audio_signals[i].t().numpy())  # 오디오 신호 플롯\n",
        "        ax.set_title(f'Label: {labels[i]}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def tic():\n",
        "    # 현재 시간을 전역 변수에 저장\n",
        "    global start_time\n",
        "    start_time = time.time()\n",
        "\n",
        "def toc():\n",
        "    # tic()이 호출된 후 경과한 시간을 계산하고 출력\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    hours = int(elapsed_time // 3600)\n",
        "    minutes = int((elapsed_time % 3600) // 60)\n",
        "    seconds = elapsed_time % 60\n",
        "    print(f\"학습에 소요된 시간은 총 : {hours}시간 {minutes}분 {seconds}초 입니다.\")"
      ],
      "metadata": {
        "id": "obc3pVbDdvth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, device, args):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
        "\n",
        "    if args.scheduler == 'multistep':\n",
        "        scheduler = MultiStepLR(optimizer, milestones=[5, 10], gamma=0.1)\n",
        "    elif args.scheduler == 'steplr':\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "    elif args.scheduler == 'reducelronplateau':\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(train_loader)\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "        all_outputs = []\n",
        "\n",
        "        for i, (audio_signals, labels) in enumerate(tqdm(train_loader)):\n",
        "            audio_signals, labels = audio_signals.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(audio_signals)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_outputs.extend(outputs.detach().cpu().numpy())  # Use detach() before numpy()\n",
        "\n",
        "        avg_loss = running_loss / total_batches\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        train_f1_score = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "        try:\n",
        "            train_auc = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
        "        except ValueError:\n",
        "            train_auc = None  # Handle the case when AUC cannot be computed\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {train_f1_score:.4f}, Train AUC: {train_auc}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val_predictions = 0\n",
        "        total_val_samples = 0\n",
        "        all_val_labels = []\n",
        "        all_val_predictions = []\n",
        "        all_val_outputs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for audio_signals, labels in tqdm(val_loader):\n",
        "                audio_signals, labels = audio_signals.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(audio_signals)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val_predictions += (predicted == labels).sum().item()\n",
        "                total_val_samples += labels.size(0)\n",
        "\n",
        "                all_val_labels.extend(labels.cpu().numpy())\n",
        "                all_val_predictions.extend(predicted.cpu().numpy())\n",
        "                all_val_outputs.extend(outputs.detach().cpu().numpy())  # Use detach() before numpy()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = correct_val_predictions / total_val_samples\n",
        "        val_f1_score = f1_score(all_val_labels, all_val_predictions, average='weighted')\n",
        "\n",
        "        try:\n",
        "            val_auc = roc_auc_score(all_val_labels, all_val_outputs, multi_class='ovr')\n",
        "        except ValueError:\n",
        "            val_auc = None  # Handle the case when AUC cannot be computed\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1_score:.4f}, Val AUC: {val_auc}\")\n",
        "\n",
        "        # Scheduler update\n",
        "        if args.scheduler == 'reducelronplateau':\n",
        "            scheduler.step(avg_val_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), f\"./model_weight_{args.data_type}.pth\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "c2KJGIAfL6Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오기\n",
        "if args.data_type == \"1d\":\n",
        "    model = Simple1DCNN()\n",
        "\n",
        "elif args.data_type == \"2d\":\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        TimeMasking(T=40, max_masks=1),\n",
        "        FrequencyMasking(F=30, max_masks=1)\n",
        "    ])\n",
        "\n",
        "    if args.model == 'vgg16':\n",
        "        model = get_model(args.model, args.n_class, pretrained=False)\n",
        "    elif args.model == 'resnet':\n",
        "        model = get_model(args.model, args.n_class, pretrained=False)\n",
        "        #quantized_model = torch.quantization.quantize_dynamic(\n",
        "            #model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8\n",
        "        #)\n",
        "        #quantized_model.eval()\n",
        "    elif args.model == 'densenet':\n",
        "        model = get_model(args.model, args.n_class, pretrained=False)\n",
        "        quantized_model = torch.quantization.quantize_dynamic(\n",
        "            model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8\n",
        "        )\n",
        "        quantized_model.eval()\n",
        "    elif args.model == 'inception_v3':\n",
        "        model = get_model(args.model, args.n_class, pretrained=False)\n",
        "        quantized_model = torch.quantization.quantize_dynamic(\n",
        "            model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8\n",
        "        )\n",
        "        quantized_model.eval()\n",
        "    elif args.model == 'simple':\n",
        "        model = Simple2DCNN()\n",
        "\n",
        "    train_dataset = ImageDataset(directory='dataset1/train_spectrograms', transform=transform)\n",
        "    val_dataset = ImageDataset(directory='dataset1/val_spectrograms', transform=transform)\n",
        "\n",
        "\n",
        "# 데이터 로더 재생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(f\"학습 데이터 수는 {len(train_dataset)}개 입니다.\")\n",
        "print(f\"검증 데이터 수는 {len(val_dataset)}개 입니다.\")"
      ],
      "metadata": {
        "id": "kIvVK0McQkms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 요약\n",
        "if args.data_type == \"1d\":\n",
        "    summary(model.cuda(), (1,16000))\n",
        "elif args.data_type == \"2d\":\n",
        "    summary(model.cuda(), (3,224,224))"
      ],
      "metadata": {
        "id": "OKiquJ2FQprc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작 및 종료에 걸린 시간을 측정하기 위한 tic - toc\n",
        "\n",
        "tic()\n",
        "\n",
        "model = train_model(model, train_loader, val_loader, epochs=args.epoch, device=device, args=args)\n",
        "\n",
        "toc()"
      ],
      "metadata": {
        "id": "4bAZFj26QrQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터에 대해 inference하기 위한 코드 입니다.\n",
        "if args.data_type == \"1d\":\n",
        "    test_dataset = AudioDataset(directory='/content/dataset/test')\n",
        "elif args.data_type == \"2d\":\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    test_dataset = ImageDataset(directory='/content/dataset/test_spectrograms', transform=transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "print(f\"테스트 데이터 수는 {len(test_dataset)}개 입니다.\")"
      ],
      "metadata": {
        "id": "Ux2tr0kGQvJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, args):\n",
        "    if args.model_path:\n",
        "        model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # F1 score 계산을 위한 레이블과 예측 값 저장\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / total * 100\n",
        "    # F1 score 계산, average='macro'\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%, Avg Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}')\n",
        "\n",
        "    return accuracy, avg_loss, f1"
      ],
      "metadata": {
        "id": "wvbhj7sBgwot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "accuracy, avg_loss, f1 = evaluate_model(model, test_loader, device=device, args=args)\n",
        "print(f\"테스트 데이터의 f1 score는 {f1}\")"
      ],
      "metadata": {
        "id": "HlMEU9RSQxZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSJk0AKUnfoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}